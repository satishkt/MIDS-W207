{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "test label shape: (677,)\n",
      "dev label shape: (676,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Example:\n",
      "0\n",
      "\n",
      "Label of Message:\n",
      "comp.graphics\n",
      "\n",
      "Text of Message:\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "- - - - -\n",
      "\n",
      "Training Example:\n",
      "1\n",
      "\n",
      "Label of Message:\n",
      "talk.religion.misc\n",
      "\n",
      "Text of Message:\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "\n",
      "- - - - -\n",
      "\n",
      "Training Example:\n",
      "2\n",
      "\n",
      "Label of Message:\n",
      "sci.space\n",
      "\n",
      "Text of Message:\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "\n",
      "- - - - -\n",
      "\n",
      "Training Example:\n",
      "3\n",
      "\n",
      "Label of Message:\n",
      "alt.atheism\n",
      "\n",
      "Text of Message:\n",
      "I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "\n",
      "- - - - -\n",
      "\n",
      "Training Example:\n",
      "4\n",
      "\n",
      "Label of Message:\n",
      "sci.space\n",
      "\n",
      "Text of Message:\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "\n",
      "- - - - -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples = 5):\n",
    "    for i in range(num_examples):\n",
    "        print \"Training Example:\\n\", i\n",
    "        print \"\\nLabel of Message:\\n\", newsgroups_train.target_names[train_labels[i]]\n",
    "        print \"\\nText of Message:\\n\", train_data[i]\n",
    "        print \"\\n- - - - -\\n\"\n",
    "P1(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a)\n",
      "Size of vocab :  26879\n",
      "Average number of non-zero features 96.0\n",
      "Fracion of non zero entries in the matrix  0.00359782722696\n",
      "b)\n",
      "0th and last feature strings are  00 and  zyxel\n",
      "c)\n",
      "Shape of the training vector with vocab of 4 words (2034, 4)\n",
      "Average number of non-zero features per example: 0.268436578171\n",
      "d)\n",
      "Size of vocabulary using analyzer = char 35478\n",
      "\n",
      "e.\n",
      "Size of vocabulary with min_df = 10: 3064\n",
      "f\n",
      "Fraction of words in dev set missing from vocabulary: 0.247876400345\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "    print \"a)\"\n",
    "    cv = CountVectorizer()\n",
    "    cv_matrix = cv.fit_transform(train_data)\n",
    "    print \"Size of vocab : \",len(cv.get_feature_names())\n",
    "    print \"Average number of non-zero features\", float((cv_matrix.nnz)/len(train_data))\n",
    "    print \"Fracion of non zero entries in the matrix \",float(cv_matrix.nnz)/(cv_matrix.shape[0] * cv_matrix.shape[1])\n",
    "    print \"b)\"\n",
    "    print \"0th and last feature strings are \",cv.get_feature_names()[0],\"and \",cv.get_feature_names()[-1]\n",
    "    print \"c)\"\n",
    "    ncv = CountVectorizer()\n",
    "    ncv.fit([\"atheism\", \"graphics\", \"space\", \"religion\"])\n",
    "    ncv_new_matrix =  ncv.transform(train_data)\n",
    "    print \"Shape of the training vector with vocab of 4 words\",ncv_new_matrix.shape\n",
    "    print \"Average number of non-zero features per example:\", float(ncv_new_matrix.nnz) / len(train_data)\n",
    "    print \"d)\"\n",
    "    cv = CountVectorizer(analyzer='char',ngram_range=(2,3))\n",
    "    cv.fit(train_data)\n",
    "    print \"Size of vocabulary using analyzer = char\",len(cv.get_feature_names())\n",
    "    # e\n",
    "    print \"\\ne.\"\n",
    "    cv = CountVectorizer(min_df=10)\n",
    "    cv.fit(train_data)\n",
    "    print \"Size of vocabulary with min_df = 10:\", len(cv.get_feature_names())\n",
    "    print \"f\"\n",
    "    cv= CountVectorizer()\n",
    "    cv.fit(train_data)\n",
    "    train_vocab = set(cv.get_feature_names())\n",
    "    cv.fit(dev_data)\n",
    "    dev_vocab = set(cv.get_feature_names())\n",
    "    print \"Fraction of words in dev set missing from vocabulary:\", float(len(dev_vocab - train_vocab)) / len(dev_vocab)\n",
    "    \n",
    "    \n",
    "    \n",
    "P2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k for K-Nearest Neighbours :  {'n_neighbors': 151}\n",
      "KNN f1 score for k = 151 is 0.402814934441\n",
      "Optimal Value for MultinomialNB = {'alpha': 0.01}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-742dda0d6474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"C setting:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\\tSum of Squared weights:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mP3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-742dda0d6474>\u001b[0m in \u001b[0;36mP3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#Calculate F1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmnb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmnb_clf_preds\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmnb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'MultinomialNB f1 score for alpha = {0} is {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnb_clf_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Satish/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    496\u001b[0m                                        dtype=np.float64)\n\u001b[1;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Satish/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc\u001b[0m in \u001b[0;36m_update_feature_log_prob\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;34m\"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0msmoothed_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0msmoothed_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothed_fc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(train_data)\n",
    "    \n",
    "    train_data_matrix = cv.transform(train_data)\n",
    "    dev_data_matrix = cv.transform(dev_data)\n",
    "    \n",
    "    \n",
    "    ##KNearest Neighbours\n",
    "    num = range(1,len(dev_data)+1)\n",
    "    num_neighbours = {'n_neighbors':num}\n",
    "    knn_clf =  KNeighborsClassifier()\n",
    "    clf = GridSearchCV(knn_clf, num_neighbours,n_jobs=3)\n",
    "    clf.fit(train_data_matrix,train_labels)\n",
    "    print \"Optimal k for K-Nearest Neighbours : \", clf.best_params_\n",
    "    \n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=clf.best_params_['n_neighbors'])\n",
    "    knn_clf.fit(train_data_matrix,train_labels)\n",
    "    knn_clf_preds = knn_clf.predict(dev_data_matrix)\n",
    "    print 'KNN f1 score for k = {0} is {1}'.format(clf.best_params_['n_neighbors'], metrics.f1_score(y_true = dev_labels, y_pred = knn_clf_preds, average=\"macro\"))\n",
    "    \n",
    "    \n",
    "    ##Multinomial NB\n",
    "    ##Find optimal alpha \n",
    "    alphas = {'alpha':[0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "    mnb_clf = MultinomialNB()\n",
    "    clf =  GridSearchCV(mnb_clf,alphas,n_jobs=3)\n",
    "    clf.fit(train_data_matrix,train_labels)\n",
    "    print \"Optimal Value for MultinomialNB =\", clf.best_params_\n",
    "    \n",
    "    #Calculate F1 score \n",
    "    mnb_clf = MultinomialNB(alpha=clf.best_params_)\n",
    "    mnb_clf.fit(train_data_matrix,train_labels)\n",
    "    mnb_clf_preds =  mnb_clf.predict(dev_data_matrix)\n",
    "    print 'MultinomialNB f1 score for alpha = {0} is {1}'.format(clf.best_params_['alpha'], metrics.f1_score(y_true = dev_labels, y_pred = mnb_clf_preds, average=\"macro\"))\n",
    "    \n",
    "    \n",
    "    ### Logistic Regression\n",
    "    # Logistic Regression\n",
    "    listofCs = [0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10]\n",
    "    Cs = {'C': listofCs}\n",
    "\n",
    "    # Calculating optimal C for Logistic Regression\n",
    "    logit_clf = LogisticRegression(penalty = 'l2')\n",
    "    clf = GridSearchCV(logit_clf, Cs,n_jobs=3)\n",
    "    clf.fit(train_data_matrix, train_labels)\n",
    "    print '\\nOptimal C for Logistic Regression =', clf.best_params_\n",
    "\n",
    "    # Calculating F1 score for optimal C\n",
    "    logit_clf = LogisticRegression(penalty = 'l2', C = clf.best_params_['C'])\n",
    "    logit_clf.fit(train_data_matrix, train_labels)\n",
    "    logit_clf_preds = logit_clf.predict(dev_data_matrix)\n",
    "    print 'Logistic Regression f1 Score for C = {0} is {1}'.format(clf.best_params_['C'], metrics.f1_score(y_true = dev_labels, y_pred = logit_clf_preds, average=\"macro\"))\n",
    "\n",
    "    \n",
    "    # Printing sum of squared weights for each class\n",
    "    print \" \"\n",
    "    for c in listofCs:\n",
    "        logit_clf = LogisticRegression(penalty = 'l2', C = c)\n",
    "        logit_clf.fit(train_data_matrix, train_labels)\n",
    "        logit_clf_preds = logit_clf.predict(dev_data_matrix)\n",
    "     \n",
    "        print \"C setting:\", c, \";\\tSum of Squared weights:\\t\", np.sum(logit_clf.coef_**2, axis=1)\n",
    "P3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> With default CountVectorizer\n",
      "> Top 5 features per class\n",
      "alt.atheism\n",
      "\t  atheists\n",
      "\t  bobby\n",
      "\t  religion\n",
      "\t  atheism\n",
      "\t  space\n",
      "comp.graphics\n",
      "\t  computer\n",
      "\t  file\n",
      "\t  image\n",
      "\t  space\n",
      "\t  graphics\n",
      "sci.space\n",
      "\t  god\n",
      "\t  nasa\n",
      "\t  orbit\n",
      "\t  graphics\n",
      "\t  space\n",
      "talk.religion.misc\n",
      "\t  order\n",
      "\t  blood\n",
      "\t  christians\n",
      "\t  christian\n",
      "\t  space\n",
      "> Table: 20 rows X 4 columns\n",
      "[[           ROW/COL        alt.atheism      comp.graphics          sci.space talk.religion.misc]\n",
      " [          atheists            0.46157           -0.07942           -0.15839           -0.29528]\n",
      " [             bobby            0.47808            -0.1204           -0.16787           -0.22783]\n",
      " [          religion            0.49395           -0.29882           -0.39323            0.00391]\n",
      " [           atheism            0.49557           -0.20723           -0.19996           -0.26777]\n",
      " [             space           -0.65519           -0.71401            1.25869           -0.59025]\n",
      " [          computer           -0.03972            0.55892           -0.32903           -0.22869]\n",
      " [              file           -0.17734            0.64123           -0.42159           -0.28827]\n",
      " [             image           -0.26363            0.64216           -0.36761           -0.21618]\n",
      " [             space           -0.65519           -0.71401            1.25869           -0.59025]\n",
      " [          graphics           -0.41108            1.00757           -0.65114           -0.37224]\n",
      " [               god            0.08549           -0.43013           -0.50772            0.27077]\n",
      " [              nasa           -0.27351           -0.26182            0.54086           -0.25311]\n",
      " [             orbit           -0.21755           -0.33417             0.5974           -0.24896]\n",
      " [          graphics           -0.41108            1.00757           -0.65114           -0.37224]\n",
      " [             space           -0.65519           -0.71401            1.25869           -0.59025]\n",
      " [             order            -0.3547           -0.03766           -0.08185            0.42908]\n",
      " [             blood           -0.20826           -0.06315           -0.09761            0.43387]\n",
      " [        christians           -0.33085           -0.16263           -0.20306            0.49944]\n",
      " [         christian           -0.26237           -0.19041           -0.19455            0.54761]\n",
      " [             space           -0.65519           -0.71401            1.25869           -0.59025]]\n",
      " \n",
      ">> With bigram features\n",
      "> Top 5 features per class\n",
      "alt.atheism\n",
      "\t  is not\n",
      "\t  in this\n",
      "\t  for the\n",
      "\t  cheers kent\n",
      "\t  looking for\n",
      "comp.graphics\n",
      "\t  that the\n",
      "\t  is there\n",
      "\t  out there\n",
      "\t  in advance\n",
      "\t  looking for\n",
      "sci.space\n",
      "\t  cheers kent\n",
      "\t  sci space\n",
      "\t  and such\n",
      "\t  the space\n",
      "\t  the moon\n",
      "talk.religion.misc\n",
      "\t  with you\n",
      "\t  does anyone\n",
      "\t  the fbi\n",
      "\t  looking for\n",
      "\t  cheers kent\n",
      "> Table: 20 rows X 4 columns\n",
      "[[           ROW/COL        alt.atheism      comp.graphics          sci.space talk.religion.misc]\n",
      " [            is not            0.29612           -0.14601           -0.28745            0.03094]\n",
      " [           in this            0.29793           -0.02633           -0.29585           -0.06522]\n",
      " [           for the           -0.30109             0.1666           -0.05251           -0.08533]\n",
      " [       cheers kent             0.3148             -0.367           -0.35533            0.32955]\n",
      " [       looking for           -0.38365            0.67123           -0.29224           -0.32773]\n",
      " [          that the            0.05299           -0.39841            0.05709            0.11453]\n",
      " [          is there           -0.17667             0.4158           -0.27461           -0.12294]\n",
      " [         out there           -0.17538            0.46299            -0.2749            -0.1638]\n",
      " [        in advance            -0.2828            0.52563           -0.25834           -0.24584]\n",
      " [       looking for           -0.38365            0.67123           -0.29224           -0.32773]\n",
      " [       cheers kent             0.3148             -0.367           -0.35533            0.32955]\n",
      " [         sci space           -0.14705           -0.20269            0.36772           -0.12417]\n",
      " [          and such           -0.12321            -0.2026             0.3702           -0.13529]\n",
      " [         the space           -0.17221           -0.28913            0.52465           -0.17163]\n",
      " [          the moon            -0.2318           -0.30901            0.56289           -0.15714]\n",
      " [          with you           -0.11611           -0.00913           -0.18209            0.27399]\n",
      " [       does anyone           -0.23446            0.29317            0.00669           -0.28846]\n",
      " [           the fbi           -0.07143           -0.11265           -0.16292            0.32027]\n",
      " [       looking for           -0.38365            0.67123           -0.29224           -0.32773]\n",
      " [       cheers kent             0.3148             -0.367           -0.35533            0.32955]]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def P4(cv,title):\n",
    "    print title\n",
    "    #cv= CountVectorizer()\n",
    "    cv.fit(train_data)\n",
    "    train_data_matrix =  cv.transform(train_data)\n",
    "    dev_data_matrix =  cv.transform(dev_data)\n",
    "    num_of_features =5\n",
    "    feature_names =  np.array(cv.get_feature_names())\n",
    "    \n",
    "    logit_clf = LogisticRegression(penalty='l2',C=0.1)\n",
    "    logit_clf.fit(train_data_matrix,train_labels)\n",
    "    \n",
    "    sortedIndices = np.argsort(np.absolute(logit_clf.coef_),axis=-1)\n",
    "    print \"> Top 5 features per class\"\n",
    "    for i in range(logit_clf.coef_.shape[0]):\n",
    "        print newsgroups_train.target_names[i]\n",
    "        for j in sortedIndices[i,-num_of_features:logit_clf.coef_.shape[1]]:\n",
    "                print \"\\t \",feature_names[j]\n",
    "    table = newsgroups_train.target_names\n",
    "    table = np.hstack(('ROW/COL', table))\n",
    "    \n",
    "    print \"> Table: 20 rows X 4 columns\"\n",
    "    for i in range(logit_clf.coef_.shape[0]):\n",
    "        for j in sortedIndices[i, -num_of_features:logit_clf.coef_.shape[1]]:\n",
    "            rounded_weights = np.round(logit_clf.coef_[:, j], 5)\n",
    "            row = np.hstack((cv.get_feature_names()[j], rounded_weights))\n",
    "            table = np.vstack((table, row))\n",
    "    np.set_printoptions(linewidth=100, formatter={'all':lambda x: '{0}'.format(str(x).rjust(18))})\n",
    "    print table\n",
    "    print \" \"\n",
    "\n",
    "                \n",
    "# With default CountVectorizer()\n",
    "cv = CountVectorizer()\n",
    "P4(cv, \">> With default CountVectorizer\")\n",
    "\n",
    "# With bigram features\n",
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "P4(cv, \">> With bigram features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  >> Without preprocessing\n",
      "\t Dictionary size : 33291\n",
      "\tf1 Score 0.702334008756\n",
      "\n",
      "  >> With preprocessing and without stop words\n",
      "\t Dictionary size : 19781\n",
      "\tf1 Score 0.710583246469\n",
      "\tReduction in dictionary size (from without preprocessing): 13510\n",
      "\tImprovement in f1-score 0.00824923771394\n",
      "\n",
      "  >> With preprocessing and stop words\n",
      "\t Dictionary size : 19554\n",
      "\tf1 Score 0.724932287752\n",
      "\tReduction in dictionary size (from without preprocessing): 13737\n",
      "\tImprovement in f1-score 0.0225982789962\n"
     ]
    }
   ],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "def better_preprocessor(s):\n",
    "    max_word_length = 6\n",
    "    s = s.lower()\n",
    "    s = re.sub('r\\d{3,}',\" numbers \",s)\n",
    "    s= re.sub('r\\W',\" \",s)\n",
    "    s= re.sub(r'_',\" \",s)\n",
    "    \n",
    "    wrds = s.split()\n",
    "    new_s = \" \"\n",
    "    for wrd in wrds:\n",
    "        if(len(wrd) >  max_word_length):\n",
    "            wrd = wrd[:max_word_length]\n",
    "        new_s = new_s + \" \"+wrd\n",
    "    return new_s\n",
    "def P5(title,t_data,t_labels,d_data,d_labels,pre_processor,stop_words):\n",
    "    print \"\\n \", title\n",
    "    cv = CountVectorizer(preprocessor=pre_processor,stop_words=stop_words)\n",
    "    cv.fit(t_data)\n",
    "    \n",
    "    t_data_matrix =  cv.transform(t_data)\n",
    "    d_data_matrix =  cv.transform(d_data)\n",
    "    \n",
    "    logit_clf =  LogisticRegression()\n",
    "    logit_clf.fit(t_data_matrix,t_labels)\n",
    "    \n",
    "    dict_size =  len(cv.get_feature_names())\n",
    "    print \"\\t Dictionary size :\" , dict_size\n",
    "    \n",
    "    logit_clf_preds =  logit_clf.predict(d_data_matrix)\n",
    "    f1= metrics.f1_score(y_true=d_labels,y_pred=logit_clf_preds,average =  'weighted')\n",
    "    print '\\tf1 Score', f1\n",
    "    \n",
    "    return (dict_size,f1)\n",
    "\n",
    "### STUDENT END ###\n",
    "title = \">> Without preprocessing\"\n",
    "stop_words = None\n",
    "no_preprocess = P5(title, train_data, train_labels, dev_data, dev_labels, empty_preprocessor, stop_words)\n",
    "\n",
    "title = \">> With preprocessing and without stop words\"\n",
    "stop_words = None\n",
    "preprocess_no_stopwords = P5(title, train_data, train_labels, dev_data, dev_labels, better_preprocessor, stop_words)\n",
    "print \"\\tReduction in dictionary size (from without preprocessing):\", no_preprocess[0] - preprocess_no_stopwords[0]\n",
    "print \"\\tImprovement in f1-score\", preprocess_no_stopwords[1] - no_preprocess[1]\n",
    "\n",
    "title = \">> With preprocessing and stop words\"\n",
    "stop_words = 'english'\n",
    "preprocess_stopwords = P5(title, train_data, train_labels, dev_data, dev_labels, better_preprocessor, stop_words)\n",
    "print \"\\tReduction in dictionary size (from without preprocessing):\", no_preprocess[0] - preprocess_stopwords[0]\n",
    "print \"\\tImprovement in f1-score\", preprocess_stopwords[1] - no_preprocess[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
